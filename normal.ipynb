{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0f837d-2b2d-47fd-8e4f-84925f017170",
   "metadata": {},
   "source": [
    "# Normal Level (Maximum Score: 100%)\n",
    "# \"Building a Reliable Predictive Model: Data Cleaning, Encoding, and MLP Classifier Optimization\"\n",
    "\n",
    "\n",
    "# Introduction\r\n",
    "\r\n",
    "In this analysis, my goal is to build a predictive model using a dataset that includes missing values, categorical variables, and numerical features that require standardization. This project focuses on effective data preparation, building an optimized model, and evaluating its performance to achieve reliable results.\r\n",
    "\r\n",
    "To start, I’ll address data preparation by handling missing values—removing rows and columns with excessive missing data and imputing values where necessary. I’ll also encode categorical variables to ensure they’re suitable for machine learning algorithms and standardize numerical features so that varying scales don’t impact model performance.\r\n",
    "\r\n",
    "For model training, I’ve chosen a Multi-Layer Perceptron (MLP) Classifier. I’ll be tuning its hyperparameters using `GridSearchCV` to balance performance with computational efficiency.\r\n",
    "\r\n",
    "Finally, I’ll evaluate the model using metrics such as accuracy, precision, recall, and F1-score. Each of these metrics will provide insights into different aspects of the model’s predictions, helping me assess its effectiveness. Through these steps, I aim to create a well-prepared, optimized, and robusies required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299d19a3-050c-4b7c-bbe6-ee0d622c24a0",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The objective of this project is to develop a reliable predictive model through systematic data preparation, model training, and evaluation. By effectively handling missing values, encoding categorical variables, and standardizing numerical features, I aim to prepare the dataset for optimal model performance. Using an MLP Classifier with hyperparameter tuning, my goal is to build a model that not only performs accurately but also generalizes well to new data. Through careful evaluation of key metrics, I aim to demonstrate the model’s strengths and identify any areas for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac105cc5-67ef-4284-9dce-bc67810cb7d2",
   "metadata": {},
   "source": [
    "### Loading the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "980cb37e-2c9d-470e-bf7a-d1f3da94abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Imputation and Preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a7432-3d57-4b1d-b581-dadc9e15f3bb",
   "metadata": {},
   "source": [
    "## Data Load:\n",
    "Loading the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "192d09e5-fa87-4378-9a0f-50fcced4894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 22 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   feature_1   900 non-null    float64\n",
      " 1   feature_2   900 non-null    float64\n",
      " 2   feature_3   900 non-null    float64\n",
      " 3   feature_4   900 non-null    float64\n",
      " 4   feature_5   900 non-null    float64\n",
      " 5   feature_6   900 non-null    float64\n",
      " 6   feature_7   900 non-null    float64\n",
      " 7   feature_8   900 non-null    float64\n",
      " 8   feature_9   900 non-null    float64\n",
      " 9   feature_10  900 non-null    float64\n",
      " 10  feature_11  900 non-null    float64\n",
      " 11  feature_12  900 non-null    float64\n",
      " 12  feature_13  900 non-null    float64\n",
      " 13  feature_14  900 non-null    float64\n",
      " 14  feature_15  900 non-null    float64\n",
      " 15  feature_16  900 non-null    float64\n",
      " 16  feature_17  900 non-null    float64\n",
      " 17  feature_18  900 non-null    float64\n",
      " 18  feature_19  900 non-null    float64\n",
      " 19  feature_20  900 non-null    float64\n",
      " 20  target      1000 non-null   int64  \n",
      " 21  category    1000 non-null   object \n",
      "dtypes: float64(20), int64(1), object(1)\n",
      "memory usage: 172.0+ KB\n",
      "Missing values per column:\n",
      "feature_1     100\n",
      "feature_2     100\n",
      "feature_3     100\n",
      "feature_4     100\n",
      "feature_5     100\n",
      "feature_6     100\n",
      "feature_7     100\n",
      "feature_8     100\n",
      "feature_9     100\n",
      "feature_10    100\n",
      "feature_11    100\n",
      "feature_12    100\n",
      "feature_13    100\n",
      "feature_14    100\n",
      "feature_15    100\n",
      "feature_16    100\n",
      "feature_17    100\n",
      "feature_18    100\n",
      "feature_19    100\n",
      "feature_20    100\n",
      "target          0\n",
      "category        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/saisa/Downloads/option1_dataset.csv')\n",
    "df.info()\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c1e662-f861-448f-ad02-6c1e403617aa",
   "metadata": {},
   "source": [
    "### Remove Rows and Columns with Too Many Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1057d8a1-31a2-4097-9a14-977fa18db6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining thresholds for row and column removal\n",
    "row_threshold = 0.5 * len(df.columns)  \n",
    "column_threshold = 0.5 * len(df)  \n",
    "\n",
    "# Remove rows with too many missing values\n",
    "df = df.dropna(thresh=row_threshold, axis=0)\n",
    "\n",
    "# Remove columns with too many missing values\n",
    "df = df.dropna(thresh=column_threshold, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5652e5c1-a519-49d1-b390-d694b8791187",
   "metadata": {},
   "source": [
    "## Impute Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "635ffef0-fc18-49ae-b579-750b104e65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Impute categorical columns with the most frequent value\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Impute numerical columns with the mean value\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "df[numerical_cols] = num_imputer.fit_transform(df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337400b-d38e-4458-a28d-aef99ab8e315",
   "metadata": {},
   "source": [
    "### Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fde35ae7-7a81-46e6-8855-ab4d20693563",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6206ea-7d03-43e5-9758-da0e1174cb84",
   "metadata": {},
   "source": [
    "### Standardize Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f60984d4-088c-47a8-a46b-0c31968bd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b54318-a799-4631-b875-098322fb31f5",
   "metadata": {},
   "source": [
    "## Step 2: Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c452f89-cbbc-45d4-bccc-78d75537385d",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f39a9538-0d41-4f39-905f-56169156fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'target' is the label column\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Perform stratified split to preserve class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ef2d0-e3f5-426c-b252-0626bfe3d97f",
   "metadata": {},
   "source": [
    "### Hyperparameter Grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85ea3841-4df9-4aa0-8e12-28c612b69d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],  # Single and double hidden layers\n",
    "    'activation': ['relu', 'tanh'],  # Commonly used activation functions\n",
    "    'solver': ['adam', 'sgd'],  # Adam is often efficient; SGD is reliable but slower\n",
    "    'learning_rate': ['constant', 'adaptive'],  # Adaptive learning rate can improve convergence\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340744b8-e099-4f29-acb5-b6b06cff13e9",
   "metadata": {},
   "source": [
    "### Hyperparameter Search Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f830f075-f723-4c01-a793-d963a5d67361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters from Random Search: {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (100,), 'activation': 'relu'}\n",
      "\n",
      "Best Model:\n",
      " MLPClassifier(max_iter=1000, random_state=42)\n",
      "\n",
      "Test Accuracy: 0.8833333333333333\n",
      "Test Precision: 0.8856250000000001\n",
      "Test Recall: 0.8833333333333333\n",
      "Test F1 Score: 0.8837922895357987\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       105\n",
      "           2       0.84      0.89      0.86        75\n",
      "\n",
      "    accuracy                           0.88       180\n",
      "   macro avg       0.88      0.88      0.88       180\n",
      "weighted avg       0.89      0.88      0.88       180\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[92 13]\n",
      " [ 8 67]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X and y are your features and target variables respectively\n",
    "\n",
    "# Step 1: Discretize the continuous labels (if needed)\n",
    "# Binning continuous labels into categories\n",
    "bins = [-np.inf, -0.5, 0.5, np.inf]  # Adjust the bin thresholds based on your data\n",
    "labels = [0, 1, 2]  # Assign labels to each bin\n",
    "y_binned = pd.cut(y, bins=bins, labels=labels)\n",
    "\n",
    "# Step 2: Split the data into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binned, test_size=0.2, stratify=y_binned, random_state=42)\n",
    "\n",
    "# Step 3: Set up the MLPClassifier model\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 4: Define the parameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'solver': ['adam', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Step 5: Set up RandomizedSearchCV with 3-fold cross-validation\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_grid, n_iter=10, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Step 6: Fit the model using RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Retrieve the best parameters and best model from RandomizedSearchCV\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and best model\n",
    "print(f\"Best Parameters from Random Search: {best_params}\")\n",
    "print(\"\\nBest Model:\\n\", best_model)\n",
    "\n",
    "# Step 8: Predict on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Step 9: Calculate and print evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nTest Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "\n",
    "# Step 10: Print the classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 11: Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6c2ad-e91b-4961-b25b-43416067483c",
   "metadata": {},
   "source": [
    "tion\r\n",
    "\r\n",
    "### Best Parameters from Random Search:\r\n",
    "The following hyperparameters were found to be the best based on the random search:\r\n",
    "\r\n",
    "- **Solver**: `adam`\r\n",
    "- **Learning Rate**: `constant`\r\n",
    "- **Hidden Layer Sizes**: `(50, 50)`\r\n",
    "- **Activation**: `relu`\r\n",
    "\r\n",
    "### Test Set Performance:\r\n",
    "After applying the best hyperparameters, the performance of the model on the test set is as follows:\r\n",
    "\r\n",
    "- **Accuracy**: 83.33%\r\n",
    "- **Precision**: 83.29%\r\n",
    "- **Recall**: 83.33%\r\n",
    "- *83     0.83      0.83     180\r\n",
    "\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1afb2d-0c02-48a9-a674-f6a711287e62",
   "metadata": {},
   "source": [
    "### Fit GridSearchCV to the training dataarams)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c3c81-00d8-47c5-99fb-8da0031a1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best parameters and best model from grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters from GridSearchCV\n",
    "print(f\"Best Parameters from Grid Search: {best_params}\")\n",
    "\n",
    "print(\"\\nBest Model:\\n\", best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12764642-57ad-4007-a5b8-1553d6c45e1e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Best Model:\n",
    "This model has been selected based on the performance of the different hyperparameter combinations evaluated during the cross-validation process. The best model and its hyperparameters are now ready to be used for predictions on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f650a2e-bae0-42ab-8a8b-80904773bf3f",
   "metadata": {},
   "source": [
    "## Evaluating the Model on Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af76389-32f2-4f1f-8268-c71e572d4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.5  \n",
    "y_pred_class = (y_pred > threshold).astype(int)\n",
    "\n",
    "# calculate the classification metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_class)\n",
    "precision = precision_score(y_test, y_pred_class, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_class, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_class, average='weighted')\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print(\"Test Precision:\", precision)\n",
    "print(\"Test Recall:\", recall)\n",
    "print(\"Test F1 Score:\", f1)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_class))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73631d1-abcd-4522-8ba6-679d61b24ce3",
   "metadata": {},
   "source": [
    "### Model Evaluation and Interpretation\r\n",
    "\r\n",
    "After evaluating the model on the test data, we can summarize the key metrics and what they mean for the model’s overall performance:\r\n",
    "\r\n",
    "#### **Test Accuracy: 82.78%**\r\n",
    "The accuracy of the model is 82.78%, meaning that the model correctly predicted the class of 82.78% of the test instances. Accuracy is a general measure of performance, but it can sometimes be misleading, especially in imbalanced datasets. In such cases, we need to dig deeper into precision, recall, and F1 scores to understand the performance across different classes.\r\n",
    "\r\n",
    "#### **Test Precision: 83.26%**\r\n",
    "Precision indicates the percentage of positive predictions made by the model that were actually correct. In this case, the model's precision is 83.26%, meaning that 83.26% of the instances predicted as positive (class 1) were truly positive. This is a good result, especially when the cost of false positives is high, as it suggests that the model doesn’t falsely classify negative instances as positive very often.\r\n",
    "\r\n",
    "#### **Test Recall: 82.78%**\r\n",
    "Recall measures the percentage of actual positive instances that were correctly identified by the model. The recall of 82.78% indicates that the model identified 82.78% of all the true positive cases in the test set. A high recall is desirable when it is important to identify as many positive cases as possible, even at the risk of making false positive predictions.\r\n",
    "\r\n",
    "#### **Test F1 Score: 82.39%**\r\n",
    "The F1 score is the harmonic mean of precision and recall. It combines the two metrics into a single value that balances both the concerns of precision and recall. The F1 score of 82.39% suggests that the model is performing reasonably well in both identifying positive instances correctly (recall) and minimizing false positives (precision). A high F1 score implies a good balance between precision and recall.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Detailed Class-wise Breakdown**\r\n",
    "\r\n",
    "#### **Class 0:**\r\n",
    "- **Precision: 81%**\r\n",
    "  - This means that when the model predicts class 0 (negative), 81% of those predictions are correct. While this is not as high as the precision for class 1, it still indicates a reasonable level of confidence in the model's negative predictions.\r\n",
    "  \r\n",
    "- **Recall: 92%**\r\n",
    "  - The recall for class 0 is significantly higher at 92%. This means the model does a great job of identifying most of the true negative instances, which is important when the cost of missing negative cases is high.\r\n",
    "\r\n",
    "- **F1-Score: 86%**\r\n",
    "  - The F1 score for class 0 is 86%, reflecting a good trade-off between precision and recall. The model is very reliable in identifying negative instances without leaving too many behind.\r\n",
    "\r\n",
    "#### **Class 1:**\r\n",
    "- **Precision: 87%**\r\n",
    "  - The precision for class 1 is 87%, which is relatively high. This indicates that the model is confident when it predicts class 1 (positive) and only makes a few false positive predictions.\r\n",
    "\r\n",
    "- **Recall: 69%**\r\n",
    "  - However, the recall for class 1 is much lower at 69%. This suggests that the model is not identifying all the true positive instances in the test set. Some positive instances are being missed, which might be a concern depending on the application. If identifying positive cases is critical (such as in disease detection or fraud detection), this low recall can be problematic.\r\n",
    "\r\n",
    "- **F1-Score: 77%**\r\n",
    "  - The F1 score for class 1 is 77%, which reflects a balance between precision and recall. While the precision is good, the relatively low recall reduces the F1 score. It indicates that the model could be improved by focusing more on capturing the positive instances.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Overall Performance Insights**\r\n",
    "The model shows a **good overall accuracy** but reveals some trade-offs when it comes to handling the two classes:\r\n",
    "- **Class 0 (Negative Class)** is well identified with a high recall (92%) and a good F1 score (86%). This suggests the model is good at predicting negatives but might be underperforming on positives.\r\n",
    "- **Class 1 (Positive Class)** sees a **good precision** (87%) but struggles with **lower recall** (69%). This means the model is missing some positive instances and could be enhanced to better identify them.\r\n",
    "\r\n",
    "### **Key Observations:**\r\n",
    "1. **Imbalance in Recall:** While the precision for both classes is relatively good, the recall for class 1 (positive) is significantly lower. The model may be more cautious about predicting positives, resulting in false negatives. In high-stakes applications like fraud detection or medical diagnoses, this could be a critical issue.\r\n",
    "   \r\n",
    "2. **Trade-off between Precision and Recall:** There is a clear trade-off between precision and recall, especially for class 1. While the model is confident in predicting positives (as seen in the high precision), it is missing a significant portion of actual positives (as shown in the lower recall). Depending on the domain, you might prioritize recall or precision differently.\r\n",
    "\r\n",
    "3. **F1 Score Reflects Balanced Performance:** The F1 scores for both classes show that the model is balancing precision and recall reasonably well, though there is room for improvement, particularly in the recall of class 1.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Next Steps for Improvement:**\r\n",
    "To improve the model's performance, especially for class 1 (positive cases), the following steps could be considered:\r\n",
    "- **Hyperparameter Tuning:** Further tuning of hyperparameters, including adjusting the threshold for classifying an instance as positive or negative, may help in improving recall for class 1.\r\n",
    "- **Class Weighting:** If the classes are imbalanced, adjusting class weights in the model could help in paying more attention to class 1, potentially improving recall at the cost of slightly lower precision.\r\n",
    "- **Advanced Models:** Trying more complex models like XGBoost or Random Forests could help in handling the class imbalance and improve the recall for class 1.\r\n",
    "- **Resampling Techniques:** Using techniques like oversampling the minority class or undersampling the majority class might also help in improving recall.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Conclusion:**\r\n",
    "The model performs reasonably well with solid precision and recall for the negative class but could be further improved in terms of identifying positive instances. Fine-tuning and experimenting with different model configurations can help address the issues observed with recall and boost the model's performance in more demanding scenarios.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2817e-f507-4f97-98b7-6302901752d4",
   "metadata": {},
   "source": [
    "### Documentation of Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e174779-1b0b-425a-a311-03dc05c8b824",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection Strategy\r\n",
    "\r\n",
    "In this project, I used `MLPClassifier` from scikit-learn as my primary model due to its capability to handle complex, non-linear decision boundaries. To optimize the model, I performed hyperparameter tuning using `GridSearchCV`, focusing on the following parameters:\r\n",
    "\r\n",
    "- `hidden_layer_sizes`: This parameter controls the architecture of the neural network. I tested different configurations (e.g., `(50,)`, `(100,)`, `(50, 50)`, `(100, 50)`) to balance model complexity and computational efficiency.\r\n",
    "- `activation`: I explored two popular activation functions, `'relu'` and `'tanh'`, as both are widely used in classification problems and offer a balance between performance and computational cost.\r\n",
    "- `solver`: The `'adam'` solver was chosen for its efficiency on relatively large datasets and its ability to adapt the learning rate.\r\n",
    "- `learning_rate`: I included both `'constant'` and `'adaptive'` learning rate options to observe if an adaptive learning rate would improve model convergence and generalization.\r\n",
    "\r\n",
    "Through 3-fold cross-validation, I was able to evaluate the performance of each parameter combination, balancing the need for computational efficiency with optimal model performance. The final chosen parameters, as seen above, represent the best configuration based on cross-validated accuracy.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca8ee5-b087-4a0c-9e21-04a63f66feec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
